# Data Preparation and Miscs

Refer to original README at [here](./README_ORIG.md).

# Reproduce Best Model Ever

```bash
reg_const=1 flip_eval=1 sing_beta=1e-7 beta=1e-3 python train_reg_crit.py \
    --root data -s market1501 -t market1501 -j 4 \
    --height 384 --width 128 --eval-freq 1 \
    --optim adam --label-smooth --lr 0.0003 --max-epoch 120 --stepsize 20 40\
    --open-layers classifier fc\
    --fixbase-epoch 10 \
    --train-batch-size 32 --test-batch-size 100 \
    -a resnet50_best \
    --save-dir path/to/your/dir \
    --gpu-devices "<gpu_id>"\
    --criterion singular \
    --switch-loss -95 \
    --regularizer svmo \
    --dropout incr \
    --data-augment crop,color-jitter,random-erase \
    --penalty-position before,after,cam,pam
```

`resnet50_best` is defined in `torchreid/models/resnet_best.py`.

Best result:

```
mAP: 85.25%
CMC curve
Rank-1  : 94.15%
Rank-5  : 98.04%
Rank-10 : 99.02%
Rank-20 : 99.32%
```

The training procedure will occupy ~10000MB video memory.

Tricks included:

 + During test time, use both original and horizontally flipped images as input. Final features is generated by averaging the two output features. (See `train_reg_crit.py::test`)
 + Random Crop, Random Erase and Color Jitter data augmentations. (See `torchreid/transforms.py`)
 + Orthogonal constraints on features. (See `torchreid/losses/singular_loss.py`, use envvar `sing_beta` as the balancing hyper-parameter)
 + Orthogonal constraints on convolution kernels. (See `torchreid/regularizers/SVMO.py`, use envvar `beta` as the balancing hyper-parameter)
 + CAM, PAM. (See `torchreid/models/tricks/attention.py`)
 + Feature distilation on middle layer of Resnet. (See `torchreid/models/tricks/feature_distilation.py`)

